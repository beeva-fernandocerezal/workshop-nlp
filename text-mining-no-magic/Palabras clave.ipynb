{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Palabras clave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras clave. 1ª versión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_keywords(words):\n",
    "    word_counts = defaultdict(lambda : 0)\n",
    "    for w in words:\n",
    "        word_counts[w]+= 1\n",
    "    return [ x for x,_ in sorted(word_counts.iteritems(), key=lambda (_,freq):freq, reverse=True)][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset URL: http://www.lsi.us.es/~fermin/corpusCine.zip\n",
    "\n",
    "Nos quedamos con los ficheros .xml, que tienen una estructura del estilo:\n",
    "\n",
    "```xml\n",
    "<review author=\"XXX\" title=\"XXX\" rank=\"X\" maxRank=\"X\" source=\"XXX\">\n",
    "\t<summary>XXX</summary>\n",
    "\t<body>XXX</body>\n",
    "</review>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs  # Know your encoding\n",
    "import re\n",
    "\n",
    "def parse_file(file_path):\n",
    "    retval = {'file': os.path.basename(file_path)}\n",
    "    with codecs.open(file_path,'r', encoding='ISO-8859-15') as fd:\n",
    "        f = fd.read().strip()\n",
    "    reg_expr = re.compile(r'\\<review author=\"(?P<author>.*)\" title=\"(?P<title>.*)\" rank=\"(?P<rank>\\d)\".*\\>\\s*<summary>(?P<summary>.*)</summary>\\s*<body>(?P<body>.*)</body>\\s*</review>')\n",
    "    regexp_result = reg_expr.search(f.strip())\n",
    "    for key in ['author', 'rank', 'title', 'summary', 'body']:\n",
    "        retval[key] = regexp_result.group(key)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_name in os.listdir(config.DATASET_MUCHOCINE_RAW):\n",
    "    try:\n",
    "        documents.append(\n",
    "            parse_file(config.DATASET_MUCHOCINE_RAW+'/'+file_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print file_name\n",
    "        print e.message\n",
    "documents.sort(key=lambda x: x['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'body': u'\"May, \\xbfQuieres ser mi amigo?\" es una de esas pel\\xedculas que nos recuerdan que el terror no siempre lleva garras de acero en una mano o una mascara en la cara. El terror y la locura se encuentran mucho m\\xe1s cerca de nosotros, de la realidad, de nuestra pac\\xedfica y hasta a veces aburrida monoton\\xeda. May funciona bajo el m\\xe9todo: la bestia duerme dentro de nosotros. En cada uno de nosotros hay un posible psic\\xf3pata y nuestra vecina la del segundo puede ocultar un oscuro pasado o una doble vida. Para ello, Lucky McKee, nos narra efectivamente una historia presuntamente cotidiana (que sin embargo, engancha desde el principio) sobre una joven muy parecida a esas otras tantas que pululan a nuestro alrededor. La rarita de la clase, la ni\\xf1a t\\xedmida de la tienda de comestibles, esa extra\\xf1a hermana de nuestro amigo, nuestra prima la del pueblo. Es por esto que nos montamos en un tren de cercan\\xedas y no en un tren de alta velocidad con escenas m\\xe1s vistas que las reposiciones de \"El Principe de Bell Air\" y adolescentes lelos e insoportables con super\\xe1vit de hormonas y d\\xe9ficit de neuronas. En este tren nos encontramos con personajes bien trazados y de carne y hueso. Personajes trabajados, atractivos e incluso muy atrevidos. (La compa\\xf1era de May en el veterinario en donde trabaja no tiene desperdicio) Todos ellos giran en torno al personaje de May, interpretado con much\\xedsimo mimo y talento por Angela Bettis que logra crear un personaje que se nos muestra irresistible y original. La trama es como digo muy interesante y nos sorprende por llevar un ritmo que rompe nuestras expectativas pero que nunca nos defrauda. La transformaci\\xf3n que sufre May y la irremediable desencadenaci\\xf3n hacia un destino sobrecogedor, sucede por culpa de toda una serie de caracter\\xedsticas y acontecimientos muy bien explicados y calculadamente reconocibles que forjan y maduran la verdadera cara del terror.Esta es una pel\\xedcula que coge carrerilla para empezar antes de donde empiezan casi todas las dem\\xe1s de su g\\xe9nero, para despu\\xe9s acabar adem\\xe1s bastante m\\xe1s all\\xe1 de donde se atrever\\xeda ninguna otra. En su d\\xeda Todd Solondz, conocido por lograr llegar tambi\\xe9n muy lejos con sus saltos y por rehuir de los g\\xe9neros habituales para inventarse los suyos propios, nos regal\\xf3 \"Bienvenidos a la casa de mu\\xf1ecas\" (1996), (pel\\xedcula anterior a su \\xe1cida y genial \"Happyness\" (1998) que tiene mucho que ver con esta. En ella, el se\\xf1or Solondz, abordaba de forma cruda y valiente, el drama de una ni\\xf1a fea y traumatizada, enfrentada a la m\\xe1s cruda realidad. La mayor diferencia entre ellas es, por supuesto, que aquella no era una pel\\xedcula de terror. Bueno, y al grano, que no esper\\xe9is escenas convencionales, ni gatos que se empe\\xf1an en perderse, ni sustitos efectistas, ni tampoco efectos especiales porque no hacen falta. Esta es una producci\\xf3n de bajo presupuesto pero adem\\xe1s no se nota. Y no se nota, porque tiene una mucho m\\xe1s que correcta fotograf\\xeda, una atm\\xf3sfera angustiosa y embriagadora, un montaje inteligente y un final muy verdadero, y por esto mismo, aterradoramente real.Esta es una peli que merece la pena, (de notable alto para quien ahora os escribe) y es que, amigos, si \"Amelie\" hubiese sido una pel\\xedcula de terror en vez de un m\\xe1gico cuento, aquella dulce francesita se hubiera llamado sin duda \"May\". \\xbfY la magia?, La magia negra, por supuesto, muy negra.', 'title': u'May', 'author': u'Ivan Sainz-Pardo', 'rank': u'4', 'summary': u'May, \\xbfquieres ser mi amigo?', 'file': '10.xml'}\n"
     ]
    }
   ],
   "source": [
    "print documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuenta palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def txt2words(txt):\n",
    "    words = txt.split(' ')\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'de', u'y', u'que', u'la', u'una', u'un', u'en', u'por', u'nos', u'no']\n"
     ]
    }
   ],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ª versión. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Taken from NLTK corpus so you don't have to download\n",
    "\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "#from nltk.corpus import stopwords\n",
    "#print filter_symbols(\n",
    "#    ' '.join(\n",
    "#        sorted(stopwords.words('spanish'))\n",
    "#    )\n",
    "#)\n",
    "\n",
    "STOPWORDS = set('''\n",
    "a al algo algunas algunos ante antes como con contra cual cuando de del desde donde durante e el ella ellas ellos\n",
    "en entre era erais eran eras eres es esa esas ese eso esos esta estaba estabais estaban estabas estad estada estadas\n",
    "estado estados estamos estando estar estaremos estara estaran estaras estare estareis estaria estariais estariamos\n",
    "estarian estarias estas este estemos esto estos estoy estuve estuviera estuvierais estuvieran estuvieras estuvieron\n",
    "estuviese estuvieseis estuviesen estuvieses estuvimos estuviste estuvisteis estuvieramos estuviesemos estuvo esta\n",
    "estabamos estais estan estas este esteis esten estes fue fuera fuerais fueran fueras fueron fuese fueseis fuesen fueses\n",
    "fui fuimos fuiste fuisteis fueramos fuesemos ha habida habidas habido habidos habiendo habremos habra habran habras\n",
    "habre habreis habria habriais habriamos habrian habrias habeis habia habiais habiamos habian habias han has hasta\n",
    "hay haya hayamos hayan hayas hayais he hemos hube hubiera hubierais hubieran hubieras hubieron hubiese hubieseis\n",
    "hubiesen hubieses hubimos hubiste hubisteis hubieramos hubiesemos hubo la las le les lo los me mi mis mucho muchos\n",
    "muy mas mi mia mias mio mios nada ni no nos nosotras nosotros nuestra nuestras nuestro nuestros o os otra otras otro\n",
    "otros para pero poco por porque que quien quienes que se sea seamos sean seas sentid sentida sentidas sentido sentidos\n",
    "seremos sera seran seras sere sereis seria seriais seriamos serian serias seais siente sin sintiendo sobre sois somos\n",
    "son soy su sus suya suyas suyo suyos si tambien tanto te tendremos tendra tendran tendras tendre tendreis tendria\n",
    "tendriais tendriamos tendrian tendrias tened tenemos tenga tengamos tengan tengas tengo tengais tenida tenidas tenido\n",
    "tenidos teniendo teneis tenia teniais teniamos tenian tenias ti tiene tienen tienes todo todos tu tus tuve tuviera\n",
    "tuvierais tuvieran tuvieras tuvieron tuviese tuvieseis tuviesen tuvieses tuvimos tuviste tuvisteis tuvieramos tuviesemos\n",
    "tuvo tuya tuyas tuyo tuyos tu un una uno unos vosostras vosostros vuestra vuestras vuestro vuestros y ya yo el eramos\n",
    "'''.split())\n",
    "\n",
    "def txt2words(txt):\n",
    "    words = [w for w in txt.split(' ') if w not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'm\\xe1s', u'La', u'En', u'terror', u'pel\\xedcula', u'tren', u'May', u'ni\\xf1a', u'adem\\xe1s', u'bien']\n"
     ]
    }
   ],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ª versión. Quitando mayúsculas, tildes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def txt2words(txt):\n",
    "    txt = txt.lower()  # Text in lowercase\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.translate(table)    \n",
    "    txt = ''.join([\n",
    "        letter for letter in txt \n",
    "        if letter in set(u'abcdefghijklmnñopqrstuvwxyz0123456789 ')]\n",
    "    )\n",
    "    words = [w for w in txt.split(' ') if w not in STOPWORDS]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'may', u'pelicula', u'terror', u'tren', u'aquella', u'solondz', u'ni\\xf1a', u'bien', u'cruda', u'cara']\n"
     ]
    }
   ],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ª versión. Raíces léxicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()\n",
    "\n",
    "def txt2words(txt):\n",
    "    txt = txt.lower()  # Text in lowercase\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.translate(table)    \n",
    "    txt = ''.join([\n",
    "        letter for letter in txt \n",
    "        if letter in set(u'abcdefghijklmnñopqrstuvwxyz0123456789 ')]\n",
    "    )\n",
    "    words = [\n",
    "        stemmer.stem(w)\n",
    "        for w in txt.split(' ')\n",
    "        if w not in STOPWORDS\n",
    "    ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'may', u'pelicul', u'personaj', u'terror', u'efect', u'tren', u'amig', u'negr', u'solondz', u'crud']\n"
     ]
    }
   ],
   "source": [
    "print get_keywords(txt2words(documents[0]['body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para todos los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in documents:\n",
    "    for key in ['body', 'summary']:\n",
    "        d[key+'_tokens'] = txt2words(d[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar/Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import zlib\n",
    "\n",
    "\n",
    "# Keys to keep\n",
    "keys_export = ['file', 'title', 'author','rank', 'summary_tokens', 'body_tokens']\n",
    "# New data structure to export\n",
    "docs_export = [\n",
    "    {key: d[key] for key in keys_export}\n",
    "    for d in documents\n",
    "]\n",
    "\n",
    "# Export\n",
    "with open(config.DATASET_MUCHOCINE, 'w+') as fd:\n",
    "    fd.write(zlib.compress(json.dumps(docs_export)))\n",
    "    \n",
    "# Import\n",
    "with open(config.DATASET_MUCHOCINE, 'r') as fd:\n",
    "    docs_import =json.loads(zlib.decompress(fd.read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
