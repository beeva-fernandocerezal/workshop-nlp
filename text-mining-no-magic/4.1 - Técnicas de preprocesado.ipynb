{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1  - Técnicas de preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset URL: http://www.lsi.us.es/~fermin/corpusCine.zip\n",
    "\n",
    "Nos quedamos con los ficheros .xml, que tienen una estructura del estilo:\n",
    "\n",
    "```xml\n",
    "<review author=\"XXX\" title=\"XXX\" rank=\"X\" maxRank=\"X\" source=\"XXX\">\n",
    "\t<summary>XXX</summary>\n",
    "\t<body>XXX</body>\n",
    "</review>\n",
    "```\n",
    "\n",
    "La codificación del texto es ISO-8859-15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os  # Utilerías del S.O.\n",
    "import re  # Expresiones regulares\n",
    "\n",
    "import config  # Ficheros de configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confidence': 0.8992448499992886, 'encoding': 'ISO-8859-2'}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "\n",
    "with open(config.DATASET_MUCHOCINE_RAW+'/11.xml', 'r') as fd:\n",
    "    txt = fd.read()\n",
    "    \n",
    "print chardet.detect(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con un parseador XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los ficheros paracen estar en formato XML.\n",
    "La opción más razonable es ingerir los ficheros con un parseador XML.\n",
    "\n",
    "Flujo de errores:\n",
    "1. El parser no reconoce los documentos como XML.\n",
    "    * Añadir la etiqueta `<?xml version=\"1.0\" encoding=\"ISO-8859-15\"?>`\n",
    "1. Hay entidades XML no reconocidas (HTML). Ver http://www.degraeve.com/reference/specialcharacters.php\n",
    "    * Añadir la etiqueta `<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"  \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">`.\n",
    "    * Añadir entidades HTML al parser.\n",
    "1. Hay documentos que no respetan la sintaxis porque incluyen el símbolo & en vez de la entidad &amp;\n",
    "    * Este ya es un problema para las expresiones regulares.\n",
    "    \n",
    "    ![title](html/2problems-regex.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import htmlentitydefs\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_from_document_parser(file_path):\n",
    "    parser = ET.XMLParser()\n",
    "    # Add HTLM entities to the parser\n",
    "    for k,v in htmlentitydefs.name2codepoint.iteritems():\n",
    "        parser.entity[k] = unichr(v)\n",
    "    retval = {'file': os.path.basename(file_path)}\n",
    "    with open(file_path,'r') as fd:\n",
    "        f = fd.read()\n",
    "    # Fixes solitary ampersands\n",
    "    f = re.sub(' & ',' &amp; ', f)\n",
    "    # Convert it into a XML file + entities definition\n",
    "    parser.feed('<?xml version=\"1.0\" encoding=\"ISO-8859-15\"?><!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"  \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">'+f)\n",
    "    xml_root = parser.close()\n",
    "    # Extract info\n",
    "    for key in {'author', 'rank', 'title'}:\n",
    "        retval[key] = xml_root.attrib.get('key')\n",
    "    retval['summary'] = xml_root.find('summary').text\n",
    "    retval['body'] = xml_root.find('body').text\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_name in os.listdir(config.DATASET_MUCHOCINE_RAW):\n",
    "    try:\n",
    "        documents.append(\n",
    "            extract_from_document_parser(config.DATASET_MUCHOCINE_RAW+'/'+file_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print file_name\n",
    "        print e.message\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(config.DATASET_MUCHOCINE_RAW+'/701.xml', 'r') as fd:\n",
    "    f = fd.read()\n",
    "f.split('\\n')[2][1376:1379]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: con [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) se puede procesar el dataset ya que no da errores. El propósito de esta sección es ser lo más agnóstico posible respecto a las herramientas y enseñar técnicas generales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con expresiones regulares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencia de Python: https://docs.python.org/2/library/re.html#regular-expression-syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs  # Know your encoding\n",
    "\n",
    "def extract_from_document_regexp(file_path):\n",
    "    retval = {'file': os.path.basename(file_path)}\n",
    "    with codecs.open(file_path,'r', encoding='ISO-8859-15') as fd:\n",
    "        f = fd.read().strip()\n",
    "    reg_expr = re.compile(r'\\<review author=\"(?P<author>.*)\" title=\"(?P<title>.*)\" rank=\"(?P<rank>\\d)\".*\\>\\s*<summary>(?P<summary>.*)</summary>\\s*<body>(?P<body>.*)</body>\\s*</review>')\n",
    "    regexp_result = reg_expr.search(f.strip())\n",
    "    for key in ['author', 'rank', 'title', 'summary', 'body']:\n",
    "        retval[key] = regexp_result.group(key)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_name in os.listdir(config.DATASET_MUCHOCINE_RAW):\n",
    "    try:\n",
    "        documents.append(\n",
    "            extract_from_document_regexp(config.DATASET_MUCHOCINE_RAW+'/'+file_name)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print file_name\n",
    "        print e.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alfabeto y conjunto de símbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_symbols(txt):\n",
    "    ALPHABET = set(u'abcdefghijklmn\\xf1opqrstuvwxyz0123456789 ')\n",
    "    table = dict(zip( #  Quitar tildes\n",
    "        [ord(x) for x in u'áéíóúü'],\n",
    "        [ord(x) for x in u'aeiouu']\n",
    "    ))\n",
    "    txt = txt.lower().translate(table)\n",
    "    txt = u''.join([x for x in txt if x in ALPHABET])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_string1 = u'Deme un puré de patatas Martínez. ¡El mejor de Sigüenza!'\n",
    "print filter_symbols(test_string1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras comunes (stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Taken from NLTK corpus so you don't have to download\n",
    "\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "#from nltk.corpus import stopwords\n",
    "#print filter_symbols(\n",
    "#    ' '.join(\n",
    "#        sorted(stopwords.words('spanish'))\n",
    "#    )\n",
    "#)\n",
    "\n",
    "STOPWORDS = set('''\n",
    "a al algo algunas algunos ante antes como con contra cual cuando de del desde donde durante e el ella ellas ellos\n",
    "en entre era erais eran eras eres es esa esas ese eso esos esta estaba estabais estaban estabas estad estada estadas\n",
    "estado estados estamos estando estar estaremos estara estaran estaras estare estareis estaria estariais estariamos\n",
    "estarian estarias estas este estemos esto estos estoy estuve estuviera estuvierais estuvieran estuvieras estuvieron\n",
    "estuviese estuvieseis estuviesen estuvieses estuvimos estuviste estuvisteis estuvieramos estuviesemos estuvo esta\n",
    "estabamos estais estan estas este esteis esten estes fue fuera fuerais fueran fueras fueron fuese fueseis fuesen fueses\n",
    "fui fuimos fuiste fuisteis fueramos fuesemos ha habida habidas habido habidos habiendo habremos habra habran habras\n",
    "habre habreis habria habriais habriamos habrian habrias habeis habia habiais habiamos habian habias han has hasta\n",
    "hay haya hayamos hayan hayas hayais he hemos hube hubiera hubierais hubieran hubieras hubieron hubiese hubieseis\n",
    "hubiesen hubieses hubimos hubiste hubisteis hubieramos hubiesemos hubo la las le les lo los me mi mis mucho muchos\n",
    "muy mas mi mia mias mio mios nada ni no nos nosotras nosotros nuestra nuestras nuestro nuestros o os otra otras otro\n",
    "otros para pero poco por porque que quien quienes que se sea seamos sean seas sentid sentida sentidas sentido sentidos\n",
    "seremos sera seran seras sere sereis seria seriais seriamos serian serias seais siente sin sintiendo sobre sois somos\n",
    "son soy su sus suya suyas suyo suyos si tambien tanto te tendremos tendra tendran tendras tendre tendreis tendria\n",
    "tendriais tendriamos tendrian tendrias tened tenemos tenga tengamos tengan tengas tengo tengais tenida tenidas tenido\n",
    "tenidos teniendo teneis tenia teniais teniamos tenian tenias ti tiene tienen tienes todo todos tu tus tuve tuviera\n",
    "tuvierais tuvieran tuvieras tuvieron tuviese tuvieseis tuviesen tuvieses tuvimos tuviste tuvisteis tuvieramos tuviesemos\n",
    "tuvo tuya tuyas tuyo tuyos tu un una uno unos vosostras vosostros vuestra vuestras vuestro vuestros y ya yo el eramos\n",
    "'''.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_string2 = u'Deme un puré de patatas Martínez. ¡El mejor de Sigüenza!'\n",
    "print [\n",
    "    word\n",
    "    for word in filter_symbols(test_string2).split(' ')\n",
    "    if word not in STOPWORDS\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raíces léxicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SpanishStemmer\n",
    "stemmer = SpanishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_string3 = u'Deme un puré de patatas Martínez. ¡El mejor de Sigüenza!'\n",
    "print [\n",
    "    stemmer.stem(word)\n",
    "    for word in filter_symbols(test_string3).split(' ')\n",
    "    if word not in STOPWORDS\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in documents:\n",
    "    for key in ['body', 'summary']:\n",
    "        d[key+'_tokens'] = [\n",
    "            stemmer.stem(word)\n",
    "            for word in filter_symbols(d[key]).split(' ')\n",
    "            if word not in STOPWORDS\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print documents[0]['summary']\n",
    "print documents[0]['summary_tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar/Importar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import zlib\n",
    "\n",
    "\n",
    "# Keys to keep\n",
    "keys_export = ['file', 'title', 'author','rank', 'summary_tokens', 'body_tokens']\n",
    "# New data structure to export\n",
    "docs_export = [\n",
    "    {key: d[key] for key in keys_export}\n",
    "    for d in documents\n",
    "]\n",
    "\n",
    "# Export\n",
    "with open(config.DATASET_MUCHOCINE, 'w+') as fd:\n",
    "    fd.write(zlib.compress(json.dumps(docs_export)))\n",
    "    \n",
    "# Import\n",
    "with open(config.DATASET_MUCHOCINE, 'r') as fd:\n",
    "    docs_import =json.loads(zlib.decompress(fd.read()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
